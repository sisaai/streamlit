import streamlit as st
import requests
import json
from datetime import datetime

# Ollama ì„œë²„ ì„¤ì •
OLLAMA_HOST = "http://127.0.0.1:11434"
MODEL_NAME = "sisaai/sisaai-llama3.1:latest"

@st.cache_data(ttl=300)
def get_ollama_version():
    try:
        response = requests.get(f"{OLLAMA_HOST}/api/version")
        return response.json().get('version', 'Unknown')
    except:
        return 'Not Available'

# ì‹œìŠ¤í…œ ì •ë³´ ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("System Information")
    ollama_ver = get_ollama_version()
    st.metric(label="Ollama Version", value=ollama_ver)
    st.divider()
    st.markdown(f"**Current Model**: `{MODEL_NAME}`")
    st.caption(f"Last Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "metrics" not in st.session_state:
    st.session_state.metrics = []
if "partial_response" not in st.session_state:
    st.session_state.partial_response = ""

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
st.title("ğŸš€ Ollama Chat Interface")
st.subheader("Advanced Monitoring Version")

# ì—°ê²° ìƒíƒœ í‘œì‹œê¸°
status_cols = st.columns(3)
with status_cols[0]:
    st.button("ğŸ”„ Refresh Connection")
with status_cols[1]:
    server_status = "âœ… Online" if ollama_ver != 'Not Available' else "âŒ Offline"
    st.markdown(f"**Server Status**: {server_status}")
with status_cols[2]:
    st.metric("Total Messages", len(st.session_state.messages))

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for idx, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and idx < len(st.session_state.metrics):
            with st.expander("Technical Details"):
                st.json(st.session_state.metrics[idx])

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("Enter your message..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì²˜ë¦¬
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Ollama API ìš”ì²­ (ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”)
    url = f"{OLLAMA_HOST}/api/chat"
    data = {
        "model": MODEL_NAME,
        "messages": st.session_state.messages,
        "stream": True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
    }

    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        metrics = {}
        
        try:
            start_time = datetime.now()
            response = requests.post(url, json=data, stream=True)
            
            if response.status_code == 200:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                for line in response.iter_lines():
                    if line:
                        chunk = json.loads(line.decode('utf-8'))
                        
                        # ë©”íŠ¸ë¦­ ì •ë³´ ìˆ˜ì§‘ (ë§ˆì§€ë§‰ ì²­í¬ì—ì„œë§Œ)
                        if chunk.get("done"):
                            metrics = {
                                "response_time": f"{(datetime.now() - start_time).total_seconds():.2f}s",
                                "model": chunk.get('model', MODEL_NAME),
                                "created_at": chunk.get('created_at'),
                                "eval_count": chunk.get('eval_count'),
                                "eval_duration": chunk.get('eval_duration')
                            }
                        
                        # ì‘ë‹µ ë‚´ìš© ëˆ„ì 
                        if chunk.get("message") and "content" in chunk["message"]:
                            full_response += chunk["message"]["content"]
                            response_placeholder.markdown(full_response + "â–Œ")
                
                # ìµœì¢… ì‘ë‹µ ì—…ë°ì´íŠ¸
                response_placeholder.markdown(full_response)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response
                })
                st.session_state.metrics.append(metrics)
                
                # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ í‘œì‹œ
                with st.expander("Latest Response Metrics"):
                    cols = st.columns(3)
                    cols[0].metric("Response Time", metrics.get('response_time', 'N/A'))
                    cols[1].metric("Token Count", metrics.get('eval_count', 'N/A'))
                    cols[2].metric("Processing Time", 
                                 f"{metrics.get('eval_duration', 0)/1e9:.2f}s" if metrics.get('eval_duration') else 'N/A')
                    
            else:
                st.error(f"API Error: {response.status_code} - {response.text}")
                
        except requests.exceptions.RequestException as e:
            st.error(f"Connection Error: {str(e)}")
            st.info("Check if Ollama server is running with `ollama serve`")

# ì‚¬ì´ë“œë°” í•˜ë‹¨ì— ì¶”ê°€ ì •ë³´ í‘œì‹œ
with st.sidebar:
    st.divider()
    if st.session_state.metrics:
        latest_metrics = st.session_state.metrics[-1]
        st.markdown("**Last Response Metrics**")
        st.write(f"â±ï¸ Response Time: {latest_metrics.get('response_time', 'N/A')}")
        st.write(f"ğŸ”¢ Tokens Generated: {latest_metrics.get('eval_count', 'N/A')}")
